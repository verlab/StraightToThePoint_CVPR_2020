{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_VDAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOJ_mAypWdu"
      },
      "source": [
        "Data \\& Code Preparation\r\n",
        "---\r\n",
        "\r\n",
        "If you want to download the code and run it by yourself in your environment, or reproduce our experiments, please follow the next steps:\r\n",
        "\r\n",
        "- ### 1. Clone the repo and install the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hha5O6IkF0Tl"
      },
      "source": [
        "! git clone https://github.com/washingtonsk8/StraightToThePoint_CVPR2020\r\n",
        "%cd StraightToThePoint_CVPR2020\r\n",
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3OYHlSzpwjW"
      },
      "source": [
        "  - ### 2. Prepare the data to train VDAN\r\n",
        "\r\n",
        "  Download \\& Organize the MSCOCO Dataset (Annotations and Images) + Download the Pretrained GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmV72SfDGyr_"
      },
      "source": [
        "# Download and extract the annotations\r\n",
        "! wget -O semantic_encoding/resources/COCO_2017/annotations_trainval2017.zip http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
        "! unzip -j semantic_encoding/resources/COCO_2017/annotations_trainval2017.zip annotations/captions_train2017.json annotations/captions_val2017.json -d semantic_encoding/resources/COCO_2017/annotations/\r\n",
        "! rm semantic_encoding/resources/COCO_2017/annotations_trainval2017.zip\r\n",
        "\r\n",
        "# Download and extract the training images\r\n",
        "! wget -O semantic_encoding/resources/COCO_2017/train2017.zip http://images.cocodataset.org/zips/train2017.zip\r\n",
        "! unzip -q semantic_encoding/resources/COCO_2017/train2017.zip -d semantic_encoding/resources/COCO_2017/\r\n",
        "! rm semantic_encoding/resources/COCO_2017/train2017.zip\r\n",
        "\r\n",
        "# Download and extract the validation images\r\n",
        "! wget -O semantic_encoding/resources/COCO_2017/val2017.zip http://images.cocodataset.org/zips/val2017.zip\r\n",
        "! unzip -q semantic_encoding/resources/COCO_2017/val2017.zip -d semantic_encoding/resources/COCO_2017/\r\n",
        "! rm semantic_encoding/resources/COCO_2017/val2017.zip\r\n",
        "\r\n",
        "# Download the Pretrained GloVe Embeddings\r\n",
        "! wget -O semantic_encoding/resources/glove.6B.zip http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "! unzip -j semantic_encoding/resources/glove.6B.zip glove.6B.300d.txt -d semantic_encoding/resources/\r\n",
        "! rm semantic_encoding/resources/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGSQj1M7pCSl"
      },
      "source": [
        "### Training VDAN\r\n",
        "\r\n",
        "To train VDAN, you first need to set up the model and train parameters (current parameters are the same as described in the paper) in the **semantic_encoding/main.py** file, then run the training script.\r\n",
        "\r\n",
        "The training script will save the model in the **semantic_encoding/models** folder.\r\n",
        "\r\n",
        "  - ### 1. Setup\r\n",
        "\r\n",
        "    ```python\r\n",
        "    model_params = {\r\n",
        "        'word_embed_size': 300,\r\n",
        "        'sent_embed_size': 1024,\r\n",
        "        'doc_embed_size': 2048,\r\n",
        "        'hidden_feat_size': 512,\r\n",
        "        'feat_embed_size': 128,\r\n",
        "        'sent_rnn_layers': 1,\r\n",
        "        'word_rnn_layers': 1,\r\n",
        "        'word_att_size': 1024,  # Same as sent_embed_size\r\n",
        "        'sent_att_size': 2048,  # Same as doc_embed_size\r\n",
        "\r\n",
        "        'use_sentence_level_attention': True,\r\n",
        "        'use_word_level_attention': True,\r\n",
        "        'use_visual_shortcut': True,  # Uses the ResNet-50 output as the first hidden state (h_0) of the document embedder Bi-GRU.\r\n",
        "    }\r\n",
        "\r\n",
        "    train_params = {\r\n",
        "\r\n",
        "        ##### Train data files #####\r\n",
        "\r\n",
        "        # COCO 2017 TODO: Download COCO 2017 and set the following folders according to your root for COCO 2017\r\n",
        "        'captions_train_fname': 'resources/COCO_2017/annotations/captions_train2017.json',  # TODO: Download the annotation file available at: http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
        "        'captions_val_fname': 'resources/COCO_2017/annotations/captions_val2017.json',  # TODO: Download the nnotation file available at: http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
        "        'train_data_path': 'resources/COCO_2017/train2017/',  # TODO: Download and unzip the folder available at http://images.cocodataset.org/zips/train2017.zip\r\n",
        "        'val_data_path': 'resources/COCO_2017/val2017/',  # Download and unzip the folder available at http://images.cocodataset.org/zips/val2017.zip\r\n",
        "\r\n",
        "        'embeddings_filename': 'resources/glove.6B.300d.txt',  # TODO: Download and unzip the file \"glove.6B.300d.txt\" from the folder \"glove.6B\" currently available at http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "        'use_fake_embeddings': False,  # Choose if you want to use fake embeddings (Tip: Activate to speed-up debugging) -- It adds random word embeddings, removing the demand of loading the embeddings.\r\n",
        "\r\n",
        "        # Choose how much data you want to use for training and validating (Tip: Use lower values to speed-up debugging)\r\n",
        "        'train_data_proportion': 1.,\r\n",
        "        'val_data_proportion': 1.,\r\n",
        "\r\n",
        "        # Training parameters (Values for the pretrained model may be different from these values below)\r\n",
        "        'max_sents': 10,  # maximum number of sentences per document\r\n",
        "        'max_words': 20,  # maximum number of words per sentence\r\n",
        "\r\n",
        "        'train_batch_size': 64,\r\n",
        "        'val_batch_size': 64,\r\n",
        "        'num_epochs': 30,\r\n",
        "        'learning_rate': 1e-5,\r\n",
        "        'learning_rate_decay': None,  # We didn't use it in our paper. But, feel free to try ;)\r\n",
        "        'decay_at_every': None,  # We didn't use it in our paper. But, feel free to try ;)\r\n",
        "        'grad_clip': None,  # clip gradients at this value. We didn't use it in our paper. But, feel free to try ;)\r\n",
        "        'finetune_semantic_model': args.model_checkpoint_filename is not None,\r\n",
        "        'model_checkpoint_filename': args.model_checkpoint_filename,\r\n",
        "\r\n",
        "        # Image transformation parameters\r\n",
        "        'resize_size': 256,\r\n",
        "        'random_crop_size': 224,\r\n",
        "        'do_random_horizontal_flip': True,\r\n",
        "\r\n",
        "        # Machine and user data\r\n",
        "        'username': getpass.getuser(),\r\n",
        "        'hostname': socket.gethostname(),\r\n",
        "\r\n",
        "        # Training process\r\n",
        "        'optimizer': 'Adam',  # We also tested with SGD -- No improvement over Adam\r\n",
        "        'criterion': nn.CosineEmbeddingLoss(0.),\r\n",
        "\r\n",
        "        'checkpoint_folder': 'models',\r\n",
        "        'log_folder': 'logs'\r\n",
        "    }\r\n",
        "    ```\r\n",
        "\r\n",
        "  - ### 2. Train\r\n",
        "\r\n",
        "    First, make sure you have `punkt` installed..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPsrvF5OOQsi"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw_HLwE7qbHB"
      },
      "source": [
        "  Finally, you're ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfksp1BfW3Fh"
      },
      "source": [
        "%cd semantic_encoding/\r\n",
        "! python main.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}